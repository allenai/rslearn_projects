model:
  class_path: rslearn.train.lightning_module.RslearnLightningModule
  init_args:
    model:
      class_path: rslearn.models.multitask.MultiTaskModel
      init_args:
        decoders:
          segment:
            - class_path: rslearn.models.unet.UNetDecoder
              init_args:
                in_channels: [[4, 128], [8, 256], [16, 512], [32, 1024]]
                out_channels: 2
                conv_layers_per_resolution: 2
            - class_path: rslearn.train.tasks.segmentation.SegmentationHead
    lr: 0.0001
    plateau: true
    plateau_factor: 0.2
    plateau_patience: 2
    plateau_min_lr: 0
    plateau_cooldown: 20
data:
  class_path: rslearn.train.data_module.RslearnDataModule
  init_args:
    path: /weka/dfive-default/rslearn-eai/datasets/solar_farm/dataset_v1/20250605/
    task:
      class_path: rslearn.train.tasks.multi_task.MultiTask
      init_args:
        tasks:
          segment:
            class_path: rslearn.train.tasks.segmentation.SegmentationTask
            init_args:
              num_classes: 2
              metric_kwargs:
                average: "micro"
              enable_f1_metric: true
              f1_metric_thresholds: [[0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]]
              remap_values: [[0, 1], [0, 255]]
        input_mapping:
          segment:
            targets: "targets"
    batch_size: 8
    num_workers: 16
    train_config:
      patch_size: 128
      tags:
        split: train
      # There are many validation patches due to the small patch size, so we set random
      # sampler to increase the training epoch size to compensate.
      sampler:
        class_path: rslearn.train.dataset.RandomSamplerFactory
        init_args:
          replacement: true
          num_samples: 16384
    val_config:
      patch_size: 128
      load_all_patches: true
      tags:
        split: val
    test_config:
      patch_size: 128
      load_all_patches: true
      tags:
        split: val
trainer:
  max_epochs: 500
  callbacks:
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        save_last: true
        monitor: val_segment/accuracy
        mode: max
rslp_project: placeholder
rslp_experiment: placeholder
