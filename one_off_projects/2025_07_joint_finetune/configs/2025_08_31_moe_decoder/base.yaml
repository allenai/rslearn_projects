model:
  class_path: rslp.helios.lightning_module.MoELightningModule
  init_args:
    model:
      class_path: rslearn.models.multitask.MultiTaskMergedModel
      init_args:
        encoder:
          - class_path: rslp.helios.model.TaskConditionedHelios
            init_args:
              checkpoint_path: "{CHECKPOINT_PATH}"
              selector: ["encoder"]
              forward_kwargs:
                patch_size: {PATCH_SIZE}
              model_overrides:
                encoder_config:
                  task_lora_kwargs:
                    use_task_lora: true
                    task_lora_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                    gen_hidden: 64
              task_embed_opts:
                type: "precomputed"
                path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/data/task_embeds___Qwen3-Embedding-8B__256d__anchor__instruct.pt
        decoders:  # Filled in by make_multidataset_config.py
        lazy_decode: true
    lr: 0.0001
    scheduler:
      class_path: rslearn.train.scheduler.CosineAnnealingScheduler
      init_args:
        T_max: 100
        eta_min: 0

data:
  class_path: rslearn.train.data_module.MultiDatasetDataModule
  init_args:
    sample_mode: random_cycle
    refill_batches: true
    num_workers: 16
    data_modules:  # Filled in by make_multidataset_config.py

trainer:
  max_epochs: 100
  callbacks:
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        every_n_epochs: 10
        save_last: true
        monitor: val_loss
        mode: min
    - class_path: rslearn.train.callbacks.adapters.ActivateLayers
      init_args:
        selectors:
          - name: lora
            at_epoch: 20
    - class_path: rslearn.train.callbacks.freeze_unfreeze.MultiStageFineTuning
      init_args:
        stages:
          - class_path: rslearn.train.callbacks.freeze_unfreeze.FTStage
            init_args:
              at_epoch: 0
              freeze_selectors: ["encoder"]
              unfreeze_selectors: ["head", "moe"]
          - class_path: rslearn.train.callbacks.freeze_unfreeze.FTStage
            init_args:
              at_epoch: 20
              freeze_selectors: ["encoder"]
              unfreeze_selectors: ["head", "lora"]
              unfreeze_lr_factor: 10
          - class_path: rslearn.train.callbacks.freeze_unfreeze.FTStage
            init_args:
              at_epoch: 30
              freeze_selectors: []
              unfreeze_selectors: ["head", "encoder"]
              unfreeze_lr_factor: 2

rslp_project: placeholder
rslp_experiment: placeholder

all_dataset_info_path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/v3_multitask/all_dataset_info.yaml
