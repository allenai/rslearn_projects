base_cfg: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/2025_09_04_loo/base.yaml
output_path:

dataset_cfgs:
 
substitutions:
  patch_size: 8
  encoder_embedding_size: 768
  helios_checkpoint_path: /weka/dfive-default/helios/checkpoints/favyen/v0.2_base_latent_mim_128_alldata_random_fixed_modality_0.5/step320000

global_overrides:
  model:
    class_path: rslearn.train.lightning_module.RslearnLightningModule
    init_args:
      restore_config:
        class_path: rslearn.train.lightning_module.RestoreConfig
        init_args:
          restore_path: 
          selector: ["state_dict"]
          remap_prefixes:
            - ["model.", ""]
          ignore_prefixes:
            - "model.decoders.FasterRCNN.0.roi_heads.box_predictor.bbox_pred.bias"
            - "model.decoders.FasterRCNN.0.roi_heads.box_predictor.cls_score.bias"
            - "model.decoders.FasterRCNN.0.roi_heads.box_predictor.bbox_pred.weight"
            - "model.decoders.FasterRCNN.0.roi_heads.box_predictor.cls_score.weight"
            - "model.trunk.task_embedding.embed.weight"
      model:
        init_args:
          trunk:
            class_path: rslearn.models.trunk.DecoderTrunk
            init_args:
              task_embedding:
                class_path: rslearn.models.task_embedding.TaskChannelEmbedding
                init_args:
                  encoder_embedding_size: 768
                  add_spatial_embed: true
              layers:
                - class_path: rslp.helios.moe.MoETransformer
                  init_args:
                    dim: 768
                    n_layers: 1
                    n_heads: 12
                    num_experts: 4
                    num_slots: 4
          encoder:
            - class_path: rslp.helios.model.TaskConditionedHelios
              init_args:
                checkpoint_path: /weka/dfive-default/helios/checkpoints/favyen/v0.2_base_latent_mim_128_alldata_random_fixed_modality_0.5/step320000
                selector: ["encoder"]
                forward_kwargs:
                  patch_size: 8
                model_overrides:
                  encoder_config:
                    task_lora_kwargs:
                      use_task_lora: true
                      task_lora_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                      gen_hidden: 64
                task_embed_opts:
                  type: "precomputed"
                  path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/data/task_embeds___Qwen3-Embedding-8B__256d__anchor__instruct__from_yaml.pt
  trainer:
    callbacks+:
      - class_path: rslearn.train.callbacks.freeze_unfreeze.MultiStageFineTuning
        init_args:
          stages:
            - class_path: rslearn.train.callbacks.freeze_unfreeze.FTStage
              init_args:
                at_epoch: 0
                freeze_selectors: ["encoder"]
                unfreeze_selectors: ["head", "moe"]
            - class_path: rslearn.train.callbacks.freeze_unfreeze.FTStage
              init_args:
                at_epoch: 20
                freeze_selectors: ["encoder"]
                unfreeze_selectors: ["head", "lora", "moe"]
                unfreeze_lr_factor: 10
            - class_path: rslearn.train.callbacks.freeze_unfreeze.FTStage
              init_args:
                at_epoch: 30
                freeze_selectors: []
                unfreeze_selectors: ["head", "lora", "moe", "encoder"]
                unfreeze_lr_factor: 10

merge_options:
  merge_heads: true
  merge_task_labels: true
  same_label_groups:
