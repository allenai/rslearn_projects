model:
  class_path: rslp.helios.lightning_module.MoELightningModule
  init_args:
    model:
      class_path: rslearn.models.multitask.MultiTaskMergedModel
      init_args:
        encoder:
          - class_path: rslp.helios.model.TaskConditionedHelios
            init_args:
              checkpoint_path: "{CHECKPOINT_PATH}"
              selector: ["encoder"]
              forward_kwargs:
                patch_size: {PATCH_SIZE}
              model_overrides:
                encoder_config:
                  task_moe_kwargs:
                    use_task_moe: true
                    task_moe_indices: [6, 7, 8, 9, 10, 11]
                    add_noise: true
                    noise_mult: 0.2
                    num_experts: 4
                    num_slots: 1
                    expert_mult: 0.5
              task_embed_opts:
                type: "precomputed"
                path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/data/task_embeds___Qwen3-Embedding-8B__256d__anchor__instruct.pt
        decoders:  # Filled in by make_multidataset_config.py
        lazy_decode: true
    lr: 0.0001
    scheduler:
      class_path: rslearn.train.scheduler.CosineAnnealingScheduler
      init_args:
        T_max: 100
        eta_min: 0

data:
  class_path: rslearn.train.data_module.MultiDatasetDataModule
  init_args:
    sample_mode: random_cycle
    refill_batches: true
    num_workers: 16
    data_modules:  # Filled in by make_multidataset_config.py

trainer:
  max_epochs: 200
  callbacks:
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        save_last: true
        monitor: val_loss
        mode: min
    - class_path: rslearn.train.callbacks.freeze_unfreeze.MultiStageFineTuning
      init_args:
        stages:
          - at_epoch: 0
            freeze_selectors: ["encoder"]
            unfreeze_selectors: ["head"]
          - at_epoch: 20
            freeze_selectors: ["encoder"]
            unfreeze_selectors: ["head", "moe"]
          - at_epoch: 40
            freeze_selectors: []
            unfreeze_selectors: ["head", "moe", "encoder"]

rslp_project: placeholder
rslp_experiment: placeholder

all_dataset_info_path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/v3_multitask/all_dataset_info.yaml