base_cfg: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/2025_09_02_final/base.yaml
output_path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/2025_09_02_final/OUT_segment.yaml

dataset_cfgs:
  - - /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/v2_pastis/basecfg_cosinelr.yaml
    - /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/v2_pastis/basecfg_helios_mm.yaml
  - - /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/v2_satlas_solar_farm_128/basecfg_cosinelr.yaml
    - /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/configs/v2_satlas_solar_farm_128/basecfg_helios_mm.yaml

global_overrides:
  model:
    class_path: rslearn.train.lightning_module.RslearnLightningModule
    init_args:
      lr: 0.00015
      scheduler:
        class_path: rslearn.train.scheduler.CosineAnnealingScheduler
        init_args:
          T_max: 50
          eta_min: 0
      model:
        init_args:
          trunk:
            class_path: rslearn.models.trunk.DecoderTrunk
            init_args:
              task_embedding:
                class_path: rslearn.models.task_embedding.TaskChannelEmbedding
                init_args:
                  encoder_embedding_size: 768
                  add_spatial_embed: true
              layers:
                - class_path: rslp.helios.moe.MoETransformer
                  init_args:
                    dim: 768
                    n_layers: 1
                    n_heads: 12
                    num_experts: 4
                    num_slots: 4
          encoder:
            - class_path: rslp.helios.model.TaskConditionedHelios
              init_args:
                checkpoint_path: "{CHECKPOINT_PATH}"
                selector: ["encoder"]
                forward_kwargs:
                  patch_size: 8
                model_overrides:
                  encoder_config:
                    task_lora_kwargs:
                      use_task_lora: true
                      task_lora_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                      gen_hidden: 64
                task_embed_opts:
                  type: "precomputed"
                  path: /weka/dfive-default/ryanp/rslearn_projects/one_off_projects/2025_07_joint_finetune/data/task_embeds___Qwen3-Embedding-8B__256d__anchor__instruct__from_yaml.pt
  trainer:
    accumulate_grad_batches: 2

merge_options:
  merge_heads: true
  merge_task_labels: true
  same_label_groups:

substitutions:
  patch_size: 8
  encoder_embedding_size: 768
  helios_checkpoint_path: /weka/dfive-default/helios/checkpoints/favyen/v0.2_base_latent_mim_128_alldata_random_fixed_modality_0.5/step320000
