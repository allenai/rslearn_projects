model:
  class_path: rslearn.train.lightning_module.RslearnLightningModule
  init_args:
    model:
      class_path: rslearn.models.multitask.MultiTaskModel
      init_args:
        decoders:
          detect_satlas_wind_turbine:
            - class_path: rslearn.models.faster_rcnn.FasterRCNN
              init_args:
                downsample_factors: [4, 8, 16, 32]
                num_channels: 128
                num_classes: 2
                anchor_sizes: [[32], [64], [128], [256]]
    lr: 0.0001
    scheduler:
      class_path: rslearn.train.scheduler.CosineAnnealingScheduler
      init_args:
        T_max: 100
        eta_min: 0
data:
  class_path: rslearn.train.data_module.RslearnDataModule
  init_args:
    path: /weka/dfive-default/rslearn-eai/datasets/wind_turbine/dataset_v1/20250605/
    task:
      class_path: rslearn.train.tasks.multi_task.MultiTask
      init_args:
        tasks:
          detect_satlas_wind_turbine:
            class_path: rslearn.train.tasks.detection.DetectionTask
            init_args:
              property_name: "category"
              classes: ["unknown", "turbine"]
              box_size: 15
              remap_values: [[0, 1], [0, 255]]
              exclude_by_center: true
              enable_map_metric: true
              enable_f1_metric: true
              f1_metric_thresholds: [[0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]]
              f1_metric_kwargs:
                cmp_mode: "distance"
                cmp_threshold: 15
                flatten_classes: true
        input_mapping:
          detect_satlas_wind_turbine:
            targets: "targets"
    batch_size: 8
    num_workers: 16
    train_config:
      patch_size: 128
      tags:
        split: train
      # There are many validation patches due to the small patch size, so we set random
      # sampler to increase the training epoch size to compensate.
      sampler:
        class_path: rslearn.train.dataset.RandomSamplerFactory
        init_args:
          replacement: true
          num_samples: 65536
    val_config:
      patch_size: 128
      tags:
        split: val
    test_config:
      patch_size: 128
      tags:
        split: val
trainer:
  max_epochs: 500
  limit_val_batches: 256
  callbacks:
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        save_last: true
        monitor: val_detect_satlas_wind_turbine/mAP
        mode: max
rslp_project: placeholder
rslp_experiment: placeholder
